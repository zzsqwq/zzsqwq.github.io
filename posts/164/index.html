<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>如何使用CenterNet做3D目标检测测试 | Zs's Blog</title>
<meta name=keywords content="CenterNet,神经网络"><meta name=description content="CenterNet—Objects as Points介绍
​CenterNet是一个anchor-free的目标检测网络，与YOLOv3相比，精度有所提升，此外他不仅能够用于2D目标检测，也能够用于人体姿态识别，3D目标检测等···
安装CenterNet
​其实安装CenterNet的过程就是一个配置环境的问题，直接跟着官方给出的这里Install.md配置一下即可，十分推荐使用Conda来管理环境，这里给出我的环境给大家参考一下：

Ubuntu = 18.04 LTS
pytorch = 1.2.0
python = 3.6.12
torchvision = 0.4.0
cuda = 10.2

​需要注意的是：

官方给出的教程里面使用的是 pytorch 0.4.1，但是我个人在实测过程中遇到了一些问题，遂安装网上的教程更改为 pytorch 1.2.0，并且需要把 ${CenterNet_Root}/src/lib/models/networks/DCNv2 中的这个DCNv2网络更改为官方的最新版。
这里使用的cuda版本最好和你的显卡匹配，之前因为显卡驱动的一些问题导致重装了电脑，根据我们学长学姐的建议，最好直接去cuda官网那边去下载deb包直接安装。
遇到环境配置问题可以先去Google一下，一般作者都在CenterNet&rsquo;s Issues中给出了回复，如果没有，可以发邮件给作者询问，当然也可以发消息/邮箱给我，大家一起探讨一下~

运行CenterNet的demo
​想要运行demo，首先要去 Model zoo 中下载一下我们需要使用的model，2D目标检测使用的是 ctdet_coco_dla_2x.pth ，人体姿态评估使用的是 multi_pose_dla_3x.pth ，下载后统一将他们放在CenterNet根目录中的model文件夹中。
​然后使用conda切换到CenterNet的环境，在终端中运行：
python demo.py ctdet --demo ${CenterNet_Root}/images/17790319373_bd19b24cfc_k.jpg --load_model ../models/ctdet_coco_dla_2x.pth 
​这里需要注意的是 --demo 后面的 ${CenterNet_Root}/images/17790319373_bd19b24cfc_k.jpg ，这里我使用的是官方给出的实例图片，它位于CenterNet根目录的images文件夹中，前面的 ${CenterNet_Root}  代表的是 CenterNet根目录，好比我的就位于 /home/zs/CenterNet 。
​如果不出意外的话效果应该如下图所示：

运行CenterNet的3D目标检测
配置数据集和模型
​我们可以直接参考官方的 DATA.md 来配置我们的数据集。
​然后到 Model zoo 下载3D检测使用的模型 ddd_3dop.pth 。
​这里说一下遇到的几个坑：


首先是配置数据集的过程中，我们需要配置的目录结构如图所示（官方给出的结构树有点模糊不清的感觉）
.
├── ImageSets_3dop
│   ├── test.txt
│   ├── train.txt
│   ├── trainval.txt
│   └── val.txt
├── ImageSets_subcnn
│   ├── test.txt
│   ├── train.txt
│   ├── trainval.txt
│   └── val.txt
└── training
       ├── calib
       ├── image_2
       └── label_2


然后去到 ${CenterNet_ROOT}/src/tools目录下，运行 python convert_kitti_to_coco.py  将 kitti 数据集转换为 coco 数据集的格式，不出意外应该会报错如下："><meta name=author content="zzsqwq"><link rel=canonical href=https://blog.zzsqwq.cn/posts/164/><meta name=google-site-verification content="G-WF7TH97J9X"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.zzsqwq.cn/images/avatar.png><link rel=icon type=image/png sizes=16x16 href=https://blog.zzsqwq.cn/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.zzsqwq.cn/images/favicon-32x32.ico><link rel=apple-touch-icon href=https://blog.zzsqwq.cn/apple-touch-icon.png><link rel=mask-icon href=https://blog.zzsqwq.cn/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.zzsqwq.cn/posts/164/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-WF7TH97J9X"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-WF7TH97J9X")}</script><meta property="og:title" content="如何使用CenterNet做3D目标检测测试"><meta property="og:description" content="CenterNet—Objects as Points介绍
​CenterNet是一个anchor-free的目标检测网络，与YOLOv3相比，精度有所提升，此外他不仅能够用于2D目标检测，也能够用于人体姿态识别，3D目标检测等···
安装CenterNet
​其实安装CenterNet的过程就是一个配置环境的问题，直接跟着官方给出的这里Install.md配置一下即可，十分推荐使用Conda来管理环境，这里给出我的环境给大家参考一下：

Ubuntu = 18.04 LTS
pytorch = 1.2.0
python = 3.6.12
torchvision = 0.4.0
cuda = 10.2

​需要注意的是：

官方给出的教程里面使用的是 pytorch 0.4.1，但是我个人在实测过程中遇到了一些问题，遂安装网上的教程更改为 pytorch 1.2.0，并且需要把 ${CenterNet_Root}/src/lib/models/networks/DCNv2 中的这个DCNv2网络更改为官方的最新版。
这里使用的cuda版本最好和你的显卡匹配，之前因为显卡驱动的一些问题导致重装了电脑，根据我们学长学姐的建议，最好直接去cuda官网那边去下载deb包直接安装。
遇到环境配置问题可以先去Google一下，一般作者都在CenterNet&rsquo;s Issues中给出了回复，如果没有，可以发邮件给作者询问，当然也可以发消息/邮箱给我，大家一起探讨一下~

运行CenterNet的demo
​想要运行demo，首先要去 Model zoo 中下载一下我们需要使用的model，2D目标检测使用的是 ctdet_coco_dla_2x.pth ，人体姿态评估使用的是 multi_pose_dla_3x.pth ，下载后统一将他们放在CenterNet根目录中的model文件夹中。
​然后使用conda切换到CenterNet的环境，在终端中运行：
python demo.py ctdet --demo ${CenterNet_Root}/images/17790319373_bd19b24cfc_k.jpg --load_model ../models/ctdet_coco_dla_2x.pth 
​这里需要注意的是 --demo 后面的 ${CenterNet_Root}/images/17790319373_bd19b24cfc_k.jpg ，这里我使用的是官方给出的实例图片，它位于CenterNet根目录的images文件夹中，前面的 ${CenterNet_Root}  代表的是 CenterNet根目录，好比我的就位于 /home/zs/CenterNet 。
​如果不出意外的话效果应该如下图所示：

运行CenterNet的3D目标检测
配置数据集和模型
​我们可以直接参考官方的 DATA.md 来配置我们的数据集。
​然后到 Model zoo 下载3D检测使用的模型 ddd_3dop.pth 。
​这里说一下遇到的几个坑：


首先是配置数据集的过程中，我们需要配置的目录结构如图所示（官方给出的结构树有点模糊不清的感觉）
.
├── ImageSets_3dop
│   ├── test.txt
│   ├── train.txt
│   ├── trainval.txt
│   └── val.txt
├── ImageSets_subcnn
│   ├── test.txt
│   ├── train.txt
│   ├── trainval.txt
│   └── val.txt
└── training
       ├── calib
       ├── image_2
       └── label_2


然后去到 ${CenterNet_ROOT}/src/tools目录下，运行 python convert_kitti_to_coco.py  将 kitti 数据集转换为 coco 数据集的格式，不出意外应该会报错如下："><meta property="og:type" content="article"><meta property="og:url" content="https://blog.zzsqwq.cn/posts/164/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-01-27T11:50:00+00:00"><meta property="article:modified_time" content="2021-01-27T11:50:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="如何使用CenterNet做3D目标检测测试"><meta name=twitter:description content="CenterNet—Objects as Points介绍
​CenterNet是一个anchor-free的目标检测网络，与YOLOv3相比，精度有所提升，此外他不仅能够用于2D目标检测，也能够用于人体姿态识别，3D目标检测等···
安装CenterNet
​其实安装CenterNet的过程就是一个配置环境的问题，直接跟着官方给出的这里Install.md配置一下即可，十分推荐使用Conda来管理环境，这里给出我的环境给大家参考一下：

Ubuntu = 18.04 LTS
pytorch = 1.2.0
python = 3.6.12
torchvision = 0.4.0
cuda = 10.2

​需要注意的是：

官方给出的教程里面使用的是 pytorch 0.4.1，但是我个人在实测过程中遇到了一些问题，遂安装网上的教程更改为 pytorch 1.2.0，并且需要把 ${CenterNet_Root}/src/lib/models/networks/DCNv2 中的这个DCNv2网络更改为官方的最新版。
这里使用的cuda版本最好和你的显卡匹配，之前因为显卡驱动的一些问题导致重装了电脑，根据我们学长学姐的建议，最好直接去cuda官网那边去下载deb包直接安装。
遇到环境配置问题可以先去Google一下，一般作者都在CenterNet&rsquo;s Issues中给出了回复，如果没有，可以发邮件给作者询问，当然也可以发消息/邮箱给我，大家一起探讨一下~

运行CenterNet的demo
​想要运行demo，首先要去 Model zoo 中下载一下我们需要使用的model，2D目标检测使用的是 ctdet_coco_dla_2x.pth ，人体姿态评估使用的是 multi_pose_dla_3x.pth ，下载后统一将他们放在CenterNet根目录中的model文件夹中。
​然后使用conda切换到CenterNet的环境，在终端中运行：
python demo.py ctdet --demo ${CenterNet_Root}/images/17790319373_bd19b24cfc_k.jpg --load_model ../models/ctdet_coco_dla_2x.pth 
​这里需要注意的是 --demo 后面的 ${CenterNet_Root}/images/17790319373_bd19b24cfc_k.jpg ，这里我使用的是官方给出的实例图片，它位于CenterNet根目录的images文件夹中，前面的 ${CenterNet_Root}  代表的是 CenterNet根目录，好比我的就位于 /home/zs/CenterNet 。
​如果不出意外的话效果应该如下图所示：

运行CenterNet的3D目标检测
配置数据集和模型
​我们可以直接参考官方的 DATA.md 来配置我们的数据集。
​然后到 Model zoo 下载3D检测使用的模型 ddd_3dop.pth 。
​这里说一下遇到的几个坑：


首先是配置数据集的过程中，我们需要配置的目录结构如图所示（官方给出的结构树有点模糊不清的感觉）
.
├── ImageSets_3dop
│   ├── test.txt
│   ├── train.txt
│   ├── trainval.txt
│   └── val.txt
├── ImageSets_subcnn
│   ├── test.txt
│   ├── train.txt
│   ├── trainval.txt
│   └── val.txt
└── training
       ├── calib
       ├── image_2
       └── label_2


然后去到 ${CenterNet_ROOT}/src/tools目录下，运行 python convert_kitti_to_coco.py  将 kitti 数据集转换为 coco 数据集的格式，不出意外应该会报错如下："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.zzsqwq.cn/posts/"},{"@type":"ListItem","position":2,"name":"如何使用CenterNet做3D目标检测测试","item":"https://blog.zzsqwq.cn/posts/164/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"如何使用CenterNet做3D目标检测测试","name":"如何使用CenterNet做3D目标检测测试","description":"CenterNet—Objects as Points介绍 ​CenterNet是一个anchor-free的目标检测网络，与YOLOv3相比，精度有所提升，此外他不仅能够用于2D目标检测，也能够用于人体姿态识别，3D目标检测等···\n安装CenterNet ​其实安装CenterNet的过程就是一个配置环境的问题，直接跟着官方给出的这里Install.md配置一下即可，十分推荐使用Conda来管理环境，这里给出我的环境给大家参考一下：\nUbuntu = 18.04 LTS\npytorch = 1.2.0\npython = 3.6.12\ntorchvision = 0.4.0\ncuda = 10.2\n​需要注意的是：\n官方给出的教程里面使用的是 pytorch 0.4.1，但是我个人在实测过程中遇到了一些问题，遂安装网上的教程更改为 pytorch 1.2.0，并且需要把 ${CenterNet_Root}/src/lib/models/networks/DCNv2 中的这个DCNv2网络更改为官方的最新版。 这里使用的cuda版本最好和你的显卡匹配，之前因为显卡驱动的一些问题导致重装了电脑，根据我们学长学姐的建议，最好直接去cuda官网那边去下载deb包直接安装。 遇到环境配置问题可以先去Google一下，一般作者都在CenterNet\u0026rsquo;s Issues中给出了回复，如果没有，可以发邮件给作者询问，当然也可以发消息/邮箱给我，大家一起探讨一下~ 运行CenterNet的demo ​想要运行demo，首先要去 Model zoo 中下载一下我们需要使用的model，2D目标检测使用的是 ctdet_coco_dla_2x.pth ，人体姿态评估使用的是 multi_pose_dla_3x.pth ，下载后统一将他们放在CenterNet根目录中的model文件夹中。\n​然后使用conda切换到CenterNet的环境，在终端中运行：\npython demo.py ctdet --demo ${CenterNet_Root}/images/17790319373_bd19b24cfc_k.jpg --load_model ../models/ctdet_coco_dla_2x.pth ​这里需要注意的是 --demo 后面的 ${CenterNet_Root}/images/17790319373_bd19b24cfc_k.jpg ，这里我使用的是官方给出的实例图片，它位于CenterNet根目录的images文件夹中，前面的 ${CenterNet_Root} 代表的是 CenterNet根目录，好比我的就位于 /home/zs/CenterNet 。\n​如果不出意外的话效果应该如下图所示：\n运行CenterNet的3D目标检测 配置数据集和模型 ​我们可以直接参考官方的 DATA.md 来配置我们的数据集。\n​然后到 Model zoo 下载3D检测使用的模型 ddd_3dop.pth 。\n​这里说一下遇到的几个坑：\n首先是配置数据集的过程中，我们需要配置的目录结构如图所示（官方给出的结构树有点模糊不清的感觉）\n. ├── ImageSets_3dop │ ├── test.txt │ ├── train.txt │ ├── trainval.txt │ └── val.txt ├── ImageSets_subcnn │ ├── test.txt │ ├── train.txt │ ├── trainval.txt │ └── val.txt └── training ├── calib ├── image_2 └── label_2 然后去到 ${CenterNet_ROOT}/src/tools目录下，运行 python convert_kitti_to_coco.py 将 kitti 数据集转换为 coco 数据集的格式，不出意外应该会报错如下：\n","keywords":["CenterNet","神经网络"],"articleBody":"CenterNet—Objects as Points介绍 ​CenterNet是一个anchor-free的目标检测网络，与YOLOv3相比，精度有所提升，此外他不仅能够用于2D目标检测，也能够用于人体姿态识别，3D目标检测等···\n安装CenterNet ​其实安装CenterNet的过程就是一个配置环境的问题，直接跟着官方给出的这里Install.md配置一下即可，十分推荐使用Conda来管理环境，这里给出我的环境给大家参考一下：\nUbuntu = 18.04 LTS\npytorch = 1.2.0\npython = 3.6.12\ntorchvision = 0.4.0\ncuda = 10.2\n​需要注意的是：\n官方给出的教程里面使用的是 pytorch 0.4.1，但是我个人在实测过程中遇到了一些问题，遂安装网上的教程更改为 pytorch 1.2.0，并且需要把 ${CenterNet_Root}/src/lib/models/networks/DCNv2 中的这个DCNv2网络更改为官方的最新版。 这里使用的cuda版本最好和你的显卡匹配，之前因为显卡驱动的一些问题导致重装了电脑，根据我们学长学姐的建议，最好直接去cuda官网那边去下载deb包直接安装。 遇到环境配置问题可以先去Google一下，一般作者都在CenterNet’s Issues中给出了回复，如果没有，可以发邮件给作者询问，当然也可以发消息/邮箱给我，大家一起探讨一下~ 运行CenterNet的demo ​想要运行demo，首先要去 Model zoo 中下载一下我们需要使用的model，2D目标检测使用的是 ctdet_coco_dla_2x.pth ，人体姿态评估使用的是 multi_pose_dla_3x.pth ，下载后统一将他们放在CenterNet根目录中的model文件夹中。\n​然后使用conda切换到CenterNet的环境，在终端中运行：\npython demo.py ctdet --demo ${CenterNet_Root}/images/17790319373_bd19b24cfc_k.jpg --load_model ../models/ctdet_coco_dla_2x.pth ​这里需要注意的是 --demo 后面的 ${CenterNet_Root}/images/17790319373_bd19b24cfc_k.jpg ，这里我使用的是官方给出的实例图片，它位于CenterNet根目录的images文件夹中，前面的 ${CenterNet_Root} 代表的是 CenterNet根目录，好比我的就位于 /home/zs/CenterNet 。\n​如果不出意外的话效果应该如下图所示：\n运行CenterNet的3D目标检测 配置数据集和模型 ​我们可以直接参考官方的 DATA.md 来配置我们的数据集。\n​然后到 Model zoo 下载3D检测使用的模型 ddd_3dop.pth 。\n​这里说一下遇到的几个坑：\n首先是配置数据集的过程中，我们需要配置的目录结构如图所示（官方给出的结构树有点模糊不清的感觉）\n. ├── ImageSets_3dop │ ├── test.txt │ ├── train.txt │ ├── trainval.txt │ └── val.txt ├── ImageSets_subcnn │ ├── test.txt │ ├── train.txt │ ├── trainval.txt │ └── val.txt └── training ├── calib ├── image_2 └── label_2 然后去到 ${CenterNet_ROOT}/src/tools目录下，运行 python convert_kitti_to_coco.py 将 kitti 数据集转换为 coco 数据集的格式，不出意外应该会报错如下：\n这里的解决方案参考CenterNet中的一个Issue , How to generate the image dir in kitti? ，我们需要回到 data/kitti 目录下手动创建一个 annotations 文件夹，然后再回去运行转换程序。转换后目录结构如下：\n. ├── annotations │ ├── kitti_3dop_train.json │ ├── kitti_3dop_val.json │ ├── kitti_subcnn_train.json │ └── kitti_subcnn_val.json ├── ImageSets_3dop │ ├── test.txt │ ├── train.txt │ ├── trainval.txt │ └── val.txt ├── ImageSets_subcnn │ ├── test.txt │ ├── train.txt │ ├── trainval.txt │ └── val.txt └── training ├── calib ├── image_2 └── label_2 然后根据官方的教程，我们需要创建一个images文件夹，然后将其 training/image_2 链接到 images/trainval，我在实际的测试中，发现此方法并不可行。参考CenterNet中的一个Issue: Evaluate kitti–AttributeError: ‘NoneType’ object has no attribute ‘shape’ ，其中 juanmed给出了解决方案：\nI had the same problem. For some reason the simlinks that are created during the data preparation process described in DATA.md are not working. So instead of creating simlinks I simply copied the actual data into the directories indicated in DATA.md. In other words the folders data/kitti/images/test and data/kitti/images/trainval do contain the actual images.\n意思就是说，我们在 images 中的图片必须都是真实的照片，而不能只是软链接过去。\n解决方案很显然，只需要在 images 文件夹中建立一个 trainval 文件夹，将 training/image_2 中的所有图像都移入其中即可。如果有test的照片，那么也照规在 images 新建一个 test 文件夹，把测试的照片移入其中即可。\n运行测试程序 接下来我们就可以根据官方给出的 GETTING_STARTED.md 来进行我们的检测了。\n即先编译一下评估工具，然后运行测试程序，但其实还是有一点点小问题。\n具体问题可以参考 Issus: kitti test: Couldn’t read: 006042.txt of ground truth.\nIssue下 lhyfst 已经给出了解决方案 ：\nThe solution is quite simple. cd data/kitti mv label_2 label_val\n​ 更改后，运行成功~\n我们应该可以在 ${CenterNet_ROOT}/exp/ddd/3dop/results 看到我们得到的结果，只不过运行得到的是点的坐标，而不是图像，如果需要图像的话可能还需要自己绘制一下。\n","wordCount":"302","inLanguage":"en","datePublished":"2021-01-27T11:50:00Z","dateModified":"2021-01-27T11:50:00Z","author":{"@type":"Person","name":"zzsqwq"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.zzsqwq.cn/posts/164/"},"publisher":{"@type":"Organization","name":"Zs's Blog","logo":{"@type":"ImageObject","url":"https://blog.zzsqwq.cn/images/avatar.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.zzsqwq.cn/ accesskey=h title="Zs's Blog (Alt + H)">Zs's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.zzsqwq.cn/about title=About><span>About</span></a></li><li><a href=https://blog.zzsqwq.cn/archives title=Posts><span>Posts</span></a></li><li><a href=https://blog.zzsqwq.cn/friends title=Friends><span>Friends</span></a></li><li><a href=https://blog.zzsqwq.cn/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.zzsqwq.cn/>Home</a>&nbsp;»&nbsp;<a href=https://blog.zzsqwq.cn/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">如何使用CenterNet做3D目标检测测试</h1><div class=post-meta><span title='2021-01-27 11:50:00 +0000 UTC'>January 27, 2021</span>&nbsp;·&nbsp;zzsqwq&nbsp;|&nbsp;<a href=https://github.com/zzsqwq/zzsqwq.github.io/tree/master/content/posts/%e5%a6%82%e4%bd%95%e4%bd%bf%e7%94%a8CenterNet%e5%81%9a3D%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b%e6%b5%8b%e8%af%95.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#centernetobjects-as-points%e4%bb%8b%e7%bb%8d aria-label="CenterNet—Objects as Points介绍">CenterNet—Objects as Points介绍</a><ul><li><a href=#%e5%ae%89%e8%a3%85centernet aria-label=安装CenterNet>安装CenterNet</a></li><li><a href=#%e8%bf%90%e8%a1%8ccenternet%e7%9a%84demo aria-label=运行CenterNet的demo>运行CenterNet的demo</a></li><li><a href=#%e8%bf%90%e8%a1%8ccenternet%e7%9a%843d%e7%9b%ae%e6%a0%87%e6%a3%80%e6%b5%8b aria-label=运行CenterNet的3D目标检测>运行CenterNet的3D目标检测</a><ul><li><a href=#%e9%85%8d%e7%bd%ae%e6%95%b0%e6%8d%ae%e9%9b%86%e5%92%8c%e6%a8%a1%e5%9e%8b aria-label=配置数据集和模型>配置数据集和模型</a></li><li><a href=#%e8%bf%90%e8%a1%8c%e6%b5%8b%e8%af%95%e7%a8%8b%e5%ba%8f aria-label=运行测试程序>运行测试程序</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h2 id=centernetobjects-as-points介绍>CenterNet—Objects as Points介绍<a hidden class=anchor aria-hidden=true href=#centernetobjects-as-points介绍>#</a></h2><p>​<a href=https://github.com/xingyizhou/CenterNet>CenterNet</a>是一个anchor-free的目标检测网络，与YOLOv3相比，精度有所提升，此外他不仅能够用于2D目标检测，也能够用于人体姿态识别，3D目标检测等···</p><h3 id=安装centernet>安装CenterNet<a hidden class=anchor aria-hidden=true href=#安装centernet>#</a></h3><p>​其实安装<a href=https://github.com/xingyizhou/CenterNet>CenterNet</a>的过程就是一个配置环境的问题，直接跟着官方给出的这里<a href=https://github.com/xingyizhou/CenterNet/blob/master/readme/INSTALL.md>Install.md</a>配置一下即可，十分推荐使用Conda来管理环境，这里给出我的环境给大家参考一下：</p><blockquote><p>Ubuntu = 18.04 LTS</p><p>pytorch = 1.2.0</p><p>python = 3.6.12</p><p>torchvision = 0.4.0</p><p>cuda = 10.2</p></blockquote><p>​需要注意的是：</p><ul><li>官方给出的教程里面使用的是 <code>pytorch 0.4.1</code>，但是我个人在实测过程中遇到了一些问题，遂安装网上的教程更改为 <code>pytorch 1.2.0</code>，并且需要把 <code>${CenterNet_Root}/src/lib/models/networks/DCNv2</code> 中的这个<a href=https://github.com/CharlesShang/DCNv2>DCNv2</a>网络更改为官方的最新版。</li><li>这里使用的cuda版本最好和你的显卡匹配，之前因为显卡驱动的一些问题导致重装了电脑，根据我们学长学姐的建议，最好直接去cuda官网那边去下载deb包直接安装。</li><li>遇到环境配置问题可以先去Google一下，一般作者都在CenterNet&rsquo;s Issues中给出了回复，如果没有，可以发邮件给作者询问，当然也可以发消息/邮箱给我，大家一起探讨一下~</li></ul><h3 id=运行centernet的demo>运行CenterNet的demo<a hidden class=anchor aria-hidden=true href=#运行centernet的demo>#</a></h3><p>​想要运行demo，首先要去 <a href=https://github.com/xingyizhou/CenterNet/blob/master/readme/MODEL_ZOO.md>Model zoo</a> 中下载一下我们需要使用的model，2D目标检测使用的是 <a href="https://drive.google.com/open?id=1pl_-ael8wERdUREEnaIfqOV_VF2bEVRT">ctdet_coco_dla_2x.pth</a> ，人体姿态评估使用的是 <a href="https://drive.google.com/open?id=1PO1Ax_GDtjiemEmDVD7oPWwqQkUu28PI">multi_pose_dla_3x.pth</a> ，下载后统一将他们放在CenterNet根目录中的model文件夹中。</p><p>​然后使用conda切换到CenterNet的环境，在终端中运行：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>python demo.py ctdet --demo <span class=si>${</span><span class=nv>CenterNet_Root</span><span class=si>}</span>/images/17790319373_bd19b24cfc_k.jpg --load_model ../models/ctdet_coco_dla_2x.pth 
</span></span></code></pre></div><p>​这里需要注意的是 <code>--demo</code> 后面的 <code>${CenterNet_Root}/images/17790319373_bd19b24cfc_k.jpg</code> ，这里我使用的是官方给出的实例图片，它位于CenterNet根目录的images文件夹中，前面的 <code>${CenterNet_Root} </code>代表的是 CenterNet根目录，好比我的就位于 <code>/home/zs/CenterNet</code> 。</p><p>​如果不出意外的话效果应该如下图所示：</p><p><img alt=2D目标检测效果 loading=lazy src=https://blog.zzsqwq.cn/usr/uploads/2021/01/2469782097.jpg></p><h3 id=运行centernet的3d目标检测>运行CenterNet的3D目标检测<a hidden class=anchor aria-hidden=true href=#运行centernet的3d目标检测>#</a></h3><h4 id=配置数据集和模型>配置数据集和模型<a hidden class=anchor aria-hidden=true href=#配置数据集和模型>#</a></h4><p>​我们可以直接参考官方的 <code>DATA.md</code> 来配置我们的数据集。</p><p>​然后到 <a href=https://github.com/xingyizhou/CenterNet/blob/master/readme/MODEL_ZOO.md>Model zoo</a> 下载3D检测使用的模型 <a href="https://drive.google.com/open?id=1znsM6E-aVTkATreDuUVxoU0ajL1az8rz">ddd_3dop.pth</a> 。</p><p>​这里说一下遇到的几个坑：</p><ul><li><p>首先是配置数据集的过程中，我们需要配置的目录结构如图所示（官方给出的结构树有点模糊不清的感觉）</p><pre tabindex=0><code>.
├── ImageSets_3dop
│   ├── test.txt
│   ├── train.txt
│   ├── trainval.txt
│   └── val.txt
├── ImageSets_subcnn
│   ├── test.txt
│   ├── train.txt
│   ├── trainval.txt
│   └── val.txt
└── training
       ├── calib
       ├── image_2
       └── label_2
</code></pre></li><li><p>然后去到 <code>${CenterNet_ROOT}/src/tools</code>目录下，运行 <code>python convert_kitti_to_coco.py </code>将 <strong>kitti</strong> 数据集转换为 <strong>coco</strong> 数据集的格式，不出意外应该会报错如下：</p></li></ul><p><img alt=转换时报错 loading=lazy src=https://blog.zzsqwq.cn/usr/uploads/2021/01/3420056939.png></p><p>这里的解决方案参考CenterNet中的一个Issue , <a href=https://github.com/xingyizhou/CenterNet/issues/54>How to generate the image dir in kitti?</a> ，我们需要回到 <code>data/kitti</code> 目录下手动创建一个 <code>annotations</code> 文件夹，然后再回去运行转换程序。转换后目录结构如下：</p><pre tabindex=0><code>.
├── annotations
│   ├── kitti_3dop_train.json
│   ├── kitti_3dop_val.json
│   ├── kitti_subcnn_train.json
│   └── kitti_subcnn_val.json
├── ImageSets_3dop
│   ├── test.txt
│   ├── train.txt
│   ├── trainval.txt
│   └── val.txt
├── ImageSets_subcnn
│   ├── test.txt
│   ├── train.txt
│   ├── trainval.txt
│   └── val.txt
└── training
       ├── calib
       ├── image_2
       └── label_2
</code></pre><ul><li><p>然后根据官方的教程，我们需要创建一个images文件夹，然后将其 <code>training/image_2</code> 链接到 <code>images/trainval</code>，我在实际的测试中，发现此方法并不可行。参考CenterNet中的一个Issue: <a href=https://github.com/xingyizhou/CenterNet/issues/575>Evaluate kitti&ndash;AttributeError: &lsquo;NoneType&rsquo; object has no attribute &lsquo;shape&rsquo;</a> ，其中 juanmed给出了解决方案：</p><blockquote><p>I had the same problem. For some reason the simlinks that are created during the data preparation process described in DATA.md are not working. So instead of creating simlinks I simply copied the actual data into the directories indicated in DATA.md. In other words the folders <code>data/kitti/images/test</code> and <code>data/kitti/images/trainval</code> do contain the actual images.</p></blockquote><p>意思就是说，我们在 <strong>images</strong> 中的图片必须都是真实的照片，而不能只是软链接过去。</p><p>解决方案很显然，只需要在 <strong>images</strong> 文件夹中建立一个 <strong>trainval</strong> 文件夹，将 <code>training/image_2</code> 中的所有图像都移入其中即可。如果有test的照片，那么也照规在 <strong>images</strong> 新建一个 <strong>test</strong> 文件夹，把测试的照片移入其中即可。</p></li></ul><h4 id=运行测试程序>运行测试程序<a hidden class=anchor aria-hidden=true href=#运行测试程序>#</a></h4><p>接下来我们就可以根据官方给出的 <a href=https://github.com/xingyizhou/CenterNet/blob/master/readme/GETTING_STARTED.md>GETTING_STARTED.md</a> 来进行我们的检测了。</p><p>即先编译一下评估工具，然后运行测试程序，但其实还是有一点点小问题。</p><p>具体问题可以参考 Issus: <a href=https://github.com/xingyizhou/CenterNet/issues/55>kitti test: Couldn&rsquo;t read: 006042.txt of ground truth.</a></p><p>Issue下 <strong>lhyfst</strong> 已经给出了解决方案 ：</p><blockquote><p>The solution is quite simple.
<code>cd data/kitti</code>
<code>mv label_2 label_val</code></p></blockquote><p>​ 更改后，运行成功~</p><p>我们应该可以在 <code>${CenterNet_ROOT}/exp/ddd/3dop/results</code> 看到我们得到的结果，只不过运行得到的是点的坐标，而不是图像，如果需要图像的话可能还需要自己绘制一下。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.zzsqwq.cn/tags/centernet/>CenterNet</a></li><li><a href=https://blog.zzsqwq.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li></ul><nav class=paginav><a class=prev href=https://blog.zzsqwq.cn/posts/169/><span class=title>« Prev</span><br><span>关于Anaconda中pip路径指向问题</span>
</a><a class=next href=https://blog.zzsqwq.cn/posts/157/><span class=title>Next »</span><br><span>Git的简易教程</span></a></nav></footer><div class=comments><script>let getTheme=window.localStorage&&window.localStorage.getItem("pref-theme");getTheme=getTheme??"preferred_color_scheme";let s=document.createElement("script");s.src="https://giscus.app/client.js",s.setAttribute("data-repo","zzsqwq/hugo-blog-comment"),s.setAttribute("data-repo-id","R_kgDOGgU1qQ"),s.setAttribute("data-category","Announcements"),s.setAttribute("data-category-id","DIC_kwDOGgU1qc4CSX1m"),s.setAttribute("data-mapping","pathname"),s.setAttribute("data-strict","0"),s.setAttribute("data-reactions-enabled","1"),s.setAttribute("data-emit-metadata","0"),s.setAttribute("data-input-position","top"),s.setAttribute("data-theme",getTheme),s.setAttribute("data-lang",""),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(s)</script></div></article></main><footer class=footer><span>&copy; 2024 <a href=https://blog.zzsqwq.cn/>Zs's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a> | <a href=http://beian.miit.gov.cn/ target=_blank>鲁ICP备2020034310号</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>function sendMessage(e){const t=document.querySelector("iframe.giscus-frame");if(!t)return;t.contentWindow.postMessage({giscus:e},"https://giscus.app")}document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light"),sendMessage({setConfig:{theme:"light"}})):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"),sendMessage({setConfig:{theme:"dark"}}))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>